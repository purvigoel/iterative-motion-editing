<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
    <title>Iterative Motion Editing with Natural Language</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.0/css/bootstrap.min.css">
    <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,500,600' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="./assets/css/styles.css">

    <link rel="apple-touch-icon" sizes="180x180" href="./assets/media/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/media/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./assets/media/favicon-16x16.png">
    <link rel="manifest" href="./assets/media/site.webmanifest">

    <meta property="og:site_name" content="Iterative Motion Editing with Natural Language" />
    <meta property="og:type" content="video.other" />
    <meta property="og:title" content="Iterative Motion Editing with Natural Language" />
    <meta property="og:description" content="Iterative Motion Editing with Natural Language, 2024." />
    <script type="module" src="https://unpkg.com/@google/model-viewer@2.0.1/dist/model-viewer.min.js"></script>
    <script src="https://polyfill.io/v3/polyfill.js?features=IntersectionObserver"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.0/js/bootstrap.bundle.min.js"></script>
    <script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd002feec.js"></script>
    <script src="./assets/scripts/main.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/sticksy/dist/sticksy.min.js"></script>
</head>

<body class="noscroll">
    <div class="button-bar text-light" style="padding-bottom: 10px; background-color: rgb(35, 35, 35);">
        <div class="container" style="max-width: 768px;">
            <h1 class="text-center">Iterative Motion Editing <br> with Natural Language</h1>
        </div>
        <div class="container" style="max-width: 768px;">
            <h3 class="text-center" stye="font-size:2rem;">SIGGRAPH 2024</h3>
        </div>
        <div class="container" style="max-width: 768px; padding-bottom: 20px;">
            <div class="row authors">
                <div class="col">
                    <h5 class="text-center"><a href="https://www.purvigoel.com/">Purvi Goel<sup>1</sup></a></h5>
                    <h6 class="text-center"></h6>
                </div>
                <div class="col">
                    <h5 class="text-center"><a href="https://wangkua1.github.io/">Kuan-Chieh Wang<sup>2</sup></a></h5>
                    <h6 class="text-center"></h6>
		        </div>
                <div class="col">
                    <h5 class="text-center"><a href="https://tml.stanford.edu/">C. Karen Liu<sup>1</sup></a></h5>
                    <h6 class="text-center"></h6>
		</div>
		<div class="col">
                    <h5 class="text-center"><a href="https://graphics.stanford.edu/~kayvonf/">Kayvon Fatahalian<sup>1</sup></a></h5>
                    <h6 class="text-center"></h6>
                </div>
            </div>
        </div>

        <div class="container" style="max-width: 768px; padding-bottom: 0px;">
            <div class="row authors">
                <div class="col">
                    <h6 style="margin-top: 10px;" class="text-center"><sup>1</sup>Stanford University, <sup>2</sup>Snap Inc.</h6>
                </div>
            </div>
        </div>
        <div class="buttons" style="margin-bottom: 8px;">
            <a class="btn text-light" role="button" href="https://arxiv.org/abs/2312.11538">
                Paper
            </a>
            <a class="btn text-light" role="button" href="">
                Code (Coming Soon!)
            </a>
            <a class="btn text-light" role="button" href="https://www.youtube.com/watch?v=ySSGd5Cy52Y">
                Video
            </a>
        </div>
    </div>
    
    <div style="background-color: rgb(255,255,255);">
        

        <div class="container" style="max-width: 768px; padding-top: 30px; padding-bottom: 0px;">
            <div class="row">
                <div class="col-md-12">
                    
                    <p>
                        <!-- <strong> -->
      			We introduce a system for using natural language to <b>conversationally</b> specify local edits to character motion. Our key idea is to cast motion editing as a two-step process: converting natural language editing instructions into Python programs that describe fine-grained editing operations with a LLM, then executing resulting operations using a constraint generation and diffusion-based motion infilling process. As an intermediate between text and joints, we define a set of kinematic <b>motion editing operators (MEOs)</b> that have well-defined semantics for how to modify specific frames of a target motion.
                        <!-- </strong> -->
                    </p>
                    

                </div>
            </div>
        </div>
        <div class="container" style="max-width: 768px; padding-top: 10px;  ">
            <div class="row">
                <div class="col-md-12">
                    <video class="specialvideo video lazy img-fluid" autoplay muted loop playsinline controls   style="width:90%; padding-left:5%">
                        <source src="./assets/video/teaser2.mp4" type="video/mp4"></source>
                    </video>
                </div>
            </div>
        </div>
        <div class="button-bar" style="padding-top: 0px; padding-bottom: 0px; background-color: rgb(255,255,255);">
            <center> <p>Jump to:</p></center>
            <div class="buttons" style="margin-bottom: 8px; margin-top: 0px;">
                <a class="btn btn-black-outline" role="button" href="#section0" style="color: black; border: 1px solid black;">
                    System
                </a>
                <a class="btn btn-black-outline" role="button" href="#section1" style="color: black; border: 1px solid black;">
                    Results: Single Edits
                </a>
                <a class="btn btn-black-outline" role="button" href="#section2" style="color: black; border: 1px solid black;">
                    Results: Iterative Edits
                </a>
                <a class="btn btn-black-outline" role="button" href="#section3" style="color: black; border: 1px solid black;">
                    FAQ/Hindsights
                </a>
            </div>
        </div>
    </div>

    <div id="section0" style="background-color: rgb(240, 240, 240);">
        <div class="container" style="padding-top: 30px; padding-bottom: 30px; text-align:center;">
            <p><b>System overview.</b> Our LLM-based parser converts natural language into Python code that describes the desired motion edit (green). Each method in the program is an MEO, defining a joint to modify, a spatial constraint (rotation/translation), and a time interval during which the constraint applies. Constraints are expressed <i>relative</i> to the properties of the source motion they are applied to. These Python programs can be executed to generate the desired motion edit (blue).</p>
        </div>
    </div>
    
    <div style="background-color: rgb(255, 255, 255);">
        <div class="container" style="padding-top: 30px; padding-bottom: 30px; text-align:center;">
            <img src="assets/images/system2.png" width="100%"">
        </div>
    </div>

    <div style="background-color: rgb(240, 240, 240);">
        <div class="container" style="padding-top: 30px; padding-bottom: 30px; text-align:center;">
            <p> <b>MEO programs</b> generated by our LLM-based parser during an iterative editing session are shown below. Notice how the LLM agent provides justifications through comments to break down its reasoning (we encouraged it to do so by giving it several example programs with these patterns via in-context learning). Also notice how context from previous edits informs programs generated for future edits. </p>
        </div>
    </div>

    <div style="background-color: rgb(255, 255, 255);">
        <div class="container" style="padding-top: 30px; padding-bottom: 30px; text-align:center;">
            <img src="assets/images/example_meos4.png" width="100%"">
        </div>
    </div>

    <div id="section1" style="background-color: rgb(240, 240, 240);">
        <div class="container" style="padding-top: 30px; padding-bottom: 30px; text-align:center;">
            <p><b>Results on single edits</b>. Our system can generate edited motions that are plausible, faithful to input instructions, and non-destructive to the original motion. The videos below show motions before and after the edit.</p>
        </div>
    </div>
    <!-- Big grid -->
    <div style="background-color: rgb(255,255,255);">
        <div class="video-row">
            <div class="col" style="padding-left:10%; padding-bottom:20px; display:flex; align-items:center;">
                <video class="specialvideo video lazy img-fluid" autoplay muted loop playsinline controls style="width:70%">
                    <source src="./assets/video/arm_swings_banner.mp4" type="video/mp4"></source>
                </video>
                <img src="assets/images/example_meo_arms2.png" style="width:30%; margin-left: 10px;">
            </div>
            <!-- <div class="col" style="padding-bottom:20px;">
                <img src="assets/images/test.png">
            </div> -->

        </div>
        <div class="video-row">
            <div class="col" style="padding-left:10%; padding-bottom:20px; display:flex; align-items:center;">
                <video class="specialvideo video lazy img-fluid" autoplay muted loop playsinline controls style="width:70%">
                    <source src="./assets/video/chamberpos-banner.mp4" type="video/mp4"></source>
                </video>
                <img src="assets/images/example_meo_chamber.png" style="width:30%; margin-left: 10px;">
            </div>
        </div>
        <div class="video-row">
            <div class="col" style="padding-left:10%; padding-bottom:20px; display:flex; align-items:center;">
                <video class="specialvideo video lazy img-fluid" autoplay muted loop playsinline controls style="width:70%">
                    <source src="./assets/video/kickout_banner.mp4" type="video/mp4"></source>
                </video>
                <img src="assets/images/example_meo_jump.png" style="width:30%; margin-left: 10px;">
            </div>
        </div>
    </div>

     <div id="section2" style="background-color: rgb(240, 240, 240);">
        <div class="container" style="padding-top: 30px; padding-bottom: 30px; text-align:center;">
            <p><b>Results on iterative edits</b>.Our system allows editing motions conversationally, which enables progressive refinement of the character's motion, allows the user to break larger editing tasks into sub-goals, and supports clarification or even adjustment of editing intent. Each video below shows separate iterative editing sessions.</p>
                   
        </div>
    </div>

    <div style="background-color: rgb(255, 255, 255); padding-top: 30px; " >
            <div class="video-row">
                <div class="col" style="padding-left:30%; padding-bottom:20px; display:grid;">
                    <video class="specialvideo video lazy img-fluid" autoplay muted loop playsinline controls style="width:100%">
                        <source src="./assets/video/kick-itr.mp4" type="video/mp4"></source>
                    </video>
                </div>
                <div class="col" style="padding-right:20%; padding-bottom:20px; display:grid;">
                    <video class="specialvideo video lazy img-fluid"  muted loop playsinline controls style="width:100%">
                        <source src="./assets/video/punch-itr2.mp4" type="video/mp4"></source>
                    </video>
                </div>
            </div>
            <div class="video-row">
                <div class="col" style="padding-left:30%; display:grid;">
                    <video class="specialvideo video lazy img-fluid" muted loop playsinline controls style="width:100%">
                        <source src="./assets/video/kick2-itr.mp4" type="video/mp4"></source>
                    </video>
                </div>
                <div class="col" style="padding-right:20%; display:grid;">
                    <video class="specialvideo video lazy img-fluid" muted loop playsinline controls style="width:100%">
                        <source src="./assets/video/squat-itr2.mp4" type="video/mp4"></source>
                    </video>
                </div>
            </div>
        </div>
    </div>

    <div id="section3" style="background-color: rgb(240, 240, 240);">
        <div class="container" style="max-width: 768px; padding-top: 30px;">
            <div class="row">
                <div class="col-md-12">
                    <h2>FAQ:</h2>

                    <p> <b>Why not just use prompt engineering? </b></p>
                     One alternative to recent motion-editing methods is to iterate on the input prompt ("prompt engineering") to the best text2motion model, e.g., changing the original prompt "a side kick" to "a high side kick" and generating from scratch. Unfortunately, it can be hard to predict how these models will interpret changes in the prompt, and they provide little guarantee that the modified motion will retain any correspondence with the original. For the inherently iterative process of character motion refinement, a system that is both predictable and non-destructive to the current motion is vital. <br><br>
                    <p> <b>Is text a good way to specify motion edits? When is a text-based interface useful?</b></p>
                     Text can be an ambiguous (and thus inefficient) way to describe precise edits--so why use text at all, instead of, e.g., a traditional keyframe animation software? For a single edit to a single joint, traditional methods may be more efficient, but we believe text is useful for iterative, <i>conversational</i> editing. Text instructions can build upon or refine previous edits, mimicking a conversation between user and character. While there's certainly a ways to go, we see our work as providing a good step in the direction of scaffolding iterative editing workflows. <br><br>
                     That being said, we believe the ideal motion editing system of the future should be multimodal: text is one tool to describe edits, but so are demonstration (e.g., with images or videos), specifics of the scene/environment, and good old kinematic joint constraints. <br><br>
                    <!-- More recent methods for motion editing have emerged in response, e.g., [<a href="https://yh2371.github.io/como/">Huang 2024</a>, <a href="https://mingyuan-zhang.github.io/projects/FineMoGen.html">Zhang 2023</a>]</p> -->
                    <p> <b>What type of edits are and are not supported?</b></p>
                    Our MEOs support kinematic editing of the main joints in the SMPL body. Physics-informed edits ("jump more forcefully") or semantic, stylization-based edits ("do that more excitedly") are not handled by our system, although we are excited about these directions. <br><br>
                    Our prompt design makes it quite straightforward to add program generation support for new MEOs: the new MEO is included as an import statement in the LLM prompt; example use(s) of the new MEO is provided in the demonstration part of the prompt. With our method, the LLM is likely to correctly target new operators without retraining due to its strong priors. For full system integration, the execution engine should implement the MEO. <br><br>

                    <p> <b>How capable is the LLM Parser in generating executable code?</b></p>
                    Quite! Given 100 editing prompts (collected by both asking ChatGPT to suggest kinematic editing instructions for sourec motion descriptions and also hand-writing editing prompts), the LLM parser successfully produced programs for 90 instructions on the first try, and an additional 7 after reflection/re-generation. Only 3 prompts failed to execute (generated instructions were not implemented by our current system, e.g., neck rotation, relative rotation, medial waist rotation). Thus, we believe the fundamental challenge of the system is generating MEO programs that embody the edit well, rather than generating valid programs. <br><br>
                    

                    <h4> <b>Hindsights?</b></h4>
                    Our method splits up constraint generation from motion generation: an LLM translates instructions into executable Python programs of MEOs; our execution engine first generates motion constraints, e.g., keyframes, from the programs, then a diffusion-based motion infilling step integrates keyframes into the source motion. <br><br>
                    We found the division of labor and introduction of an IR to be quite useful when scaling up the system, as opposed to using a single model to handle the whole text-to-edited-motion pipeline. First, the MEO intermediate representation is highly interpretable: it's clear what edit is being executed, and why. Second, the representation is controllable: each MEO reduces edits to a change along a single DOF; though our examples have the magnitude of change being set procedurally, it can in principle be set directly by the user (like a single-use rig). Third, the division of labor allows the pipeline to be more modular; the infilling model does not require re-training to add new types of kinematic edits. We found, for example, that scaling up our original set of rotation/translation MEOs to also handle relative translation was fairly simple, with no fine-tuning required. <br><br>
                    The cons of this approach is that edits are currently limited to ones that can be expressed by frame-wise motion constraints and transitions in and out of those frames; as mentioned earlier, stylization-based edits are more difficult within this division of labor. Motion quality may also suffer with this more modular approach (see, in comparison, the more recent work <a href="https://yh2371.github.io/como/">[Huang 2024]</a>, which uses a single model to encapsulate both constraints and motion generation rather than our proposed two-stage execution engine). 
                    We believe there's a lot of room to explore the right balance between modularity and generation quality!<br><br>
                </div> 
            </div>
        </div>
    </div>

    <div style="background-color: rgb(30, 30, 30);">
        <div class="container text-light" style="max-width: 768px; padding: 30px 0px;">
            <div class="row">
                <div class="col-md-12">
                    <h2>Acknowledgements:</h2>
                    Purvi Goel is supported by a Stanford Interdisciplinary Graduate Fellowship. Kuan-Chieh Wang was supported by Stanford Wu-Tsai Human Performance Alliances while at Stanford University. We thank the anonymous reviewers for constructive feedback; Vishnu Sarukkai, Sarah Jobalia, Sofia Di Toro Wyetzner, Haotian Zhang, David Durst, and James Hong for helpful discussions. Our codebase was built with invaluable help from <a href="https://jmhb0.github.io/">James Burgess</a>. This website was developed referencing <a href="https://edge-dance.github.io/">EDGE</a>, and by extension, the excellent <a href="https://imagen.research.google/">Imagen</a> site.
                </div>
            </div>
        </div>
    </div>
</body>

</html>
